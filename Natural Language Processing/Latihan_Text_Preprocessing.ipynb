{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pendahuluan"
      ],
      "metadata": {
        "id": "zCWsWMkIfQE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pra-pemrosesan teks adalah tahap penting dalam analisis teks dan Natural Language Processing (NLP). Tujuannya adalah untuk membersihkan dan mempersiapkan teks mentah agar dapat diolah lebih lanjut dengan algoritma pemrosesan teks atau analisis.\n",
        "\n",
        "Berikut adalah beberapa langkah umum (tapi tidak terbatas pada hal di bawah) dalam pra-pemrosesan teks dalam NLP:\n",
        "\n",
        "**1. Case Folding**\n",
        "\n",
        "**2. Removal Special Characters**\n",
        "- Menghapus Angka\n",
        "- Menghapus Tanda Baca\n",
        "- Menghapus White Space\n",
        "  - Menggunakan strip() untuk Menghapus Whitespace di Awal dan Akhir\n",
        "  - Menggunakan replace() untuk Menghapus Whitespace di Seluruh String\n",
        "\n",
        "**3. Stopword Removal (Filtering)**\n",
        "-  Stopwords NLTK (Natural Language Toolkit)\n",
        "- Stopwords Sastrawi\n",
        "\n",
        "**4. Tokenizing**\n",
        "- Tokenisasi Kata (Word Tokenization)\n",
        "- Tokenisasi Kalimat (Sentence Tokenization)\n",
        "- Tokenisasi Frasa (Phrase Tokenization)\n",
        "- Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)\n",
        "- Tokenisasi Berdasarkan Model (Model-based Tokenization)\n",
        "\n",
        "**5. Stemming**\n",
        "\n",
        "**6. Lemmatization**"
      ],
      "metadata": {
        "id": "sAgHJn-NfT39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "IDUF4pnNfV84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8cTLjEn2eYIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c728d64e-e644-4ba9-ed40-0124e0131454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Folding"
      ],
      "metadata": {
        "id": "Fzf4GW2KhMwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case folding adalah proses standarisasi teks dengan mengubah semua huruf dalam teks menjadi huruf kecil. Tujuan utamanya adalah untuk menciptakan konsistensi dalam representasi teks, sehingga mempermudah dalam analisis teks selanjutnya.\n",
        "\n",
        "Dengan mellakukan case folding, perbedaan huruf besar dan kecil dalam teks diabaikan, sehingga memungkinkan perbandingan dan ppecarian teks menjadi lebih mudah dan efisien."
      ],
      "metadata": {
        "id": "zHrVB_MghQYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh teks\n",
        "teks_asli = \"Ini Adalah COntoh Teks yang Akan Dikonversi Menjadi Lowercase.\"\n",
        "\n",
        "# Mengubah teks menjadi lowercase\n",
        "teks_lowercase = teks_asli.lower()\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Teks asli:\", teks_asli)\n",
        "print(\"Teks setelah diubah menjadi lowercase:\", teks_lowercase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZSs9AmfjFN",
        "outputId": "9481b9aa-0114-40d4-c718-32164b41139f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Ini Adalah COntoh Teks yang Akan Dikonversi Menjadi Lowercase.\n",
            "Teks setelah diubah menjadi lowercase: ini adalah contoh teks yang akan dikonversi menjadi lowercase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh lain\n",
        "# Contoh teks dengan campuran huruf besar dan kecil\n",
        "teks_asli = \"\"\"\n",
        "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
        "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
        "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
        "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
        "\"\"\"\n",
        "\n",
        "# Mengubah teks menjadi lowercase menggunakan case folding\n",
        "teks_case_folded = teks_asli.lower()\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Tekas asli:\")\n",
        "print(teks_asli)\n",
        "print(\"Teks setelah di case folding:\")\n",
        "print(teks_case_folded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7gLLbEqiFvZ",
        "outputId": "f2406431-f672-46df-82fc-dec72219d8a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekas asli:\n",
            "\n",
            "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
            "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
            "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
            "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
            "\n",
            "Teks setelah di case folding:\n",
            "\n",
            "ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
            "contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
            "dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
            "ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removal Special Chracters"
      ],
      "metadata": {
        "id": "UaLInOZJpqBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menghapus Angka"
      ],
      "metadata": {
        "id": "QkZQvuXwpteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghapus angka dari teks\n",
        "def hapus_angka(teks):\n",
        "  teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])\n",
        "  return teks_tanpa_angka\n",
        "\n",
        "\n",
        "# Contoh teks dengan angka\n",
        "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
        "\n",
        "# Memanggil fungsi untuk menghapus angka\n",
        "teks_tanpa_angka = hapus_angka(teks_dengan_angka)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Teks asli:\", teks_dengan_angka)\n",
        "print(\"Teks setelah dihapus angka:\", teks_tanpa_angka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB2Y5Pw5pzq1",
        "outputId": "f5a12086-c3de-4b48-bed1-2d31d6de71cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
            "Teks setelah dihapus angka: Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# contoh lain\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_numbers(text):\n",
        "  # Menggunakan regular expression untuk menghapus angka\n",
        "  text_without_numbers = re.sub(r'\\d+', '', text)\n",
        "  return text_without_numbers\n",
        "\n",
        "# Contoh teks dengan angka\n",
        "teks_asli = \"Ini adalah contoh teks dengan angka 123 dan 456.\"\n",
        "\n",
        "# Menghapus angka dari teks\n",
        "teks_tanpa_angka = remove_numbers(teks_asli)\n",
        "\n",
        "# Menampilkna hasil\n",
        "print(\"Teks asli:\", teks_asli)\n",
        "print(\"Teks setelah dihapus angka:\", teks_tanpa_angka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjJaX-MNrBdf",
        "outputId": "27486717-3193-473b-eeed-54c5f4165a04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Ini adalah contoh teks dengan angka 123 dan 456.\n",
            "Teks setelah dihapus angka: Ini adalah contoh teks dengan angka  dan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh lain\n",
        "def hapus_angka_tidak_relevan(teks):\n",
        "  # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan\n",
        "  # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon\n",
        "  pola_angka_tidak_relevan = r\"\\b(?:\\d{1,3}[-\\.\\s]?)?(?:\\d{3}[-\\.\\s]?)?\\d{4,}\\b\"\n",
        "  hasil = re.sub(pola_angka_tidak_relevan, \"\", teks)\n",
        "  return hasil.strip()\n",
        "\n",
        "# Contoh kalimat dengan angka\n",
        "kalimat = \"Di sini ada beberapa nomor rumah yaitu 123, 456, dan 789. Silahkan hubungi 085696462825 untuk informasi lebih lanjut.\"\n",
        "\n",
        "# Memanggil fungsi untuk menghapus angka tidak relevan\n",
        "hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Kalimat dengan angka\", kalimat)\n",
        "print(\"Kalimat tanpa angka\", hasil_tanpa_angka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOLgMFDorn1i",
        "outputId": "55463880-f6c1-465c-cd27-eb86b5340422"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat dengan angka Di sini ada beberapa nomor rumah yaitu 123, 456, dan 789. Silahkan hubungi 085696462825 untuk informasi lebih lanjut.\n",
            "Kalimat tanpa angka Di sini ada beberapa nomor rumah yaitu 123, 456, dan 789. Silahkan hubungi  untuk informasi lebih lanjut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menghapus Tanda Baca"
      ],
      "metadata": {
        "id": "RK-dVgmttFjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  # Membuat set yang berisi semua tanda baca\n",
        "  punctuation_set = set(string.punctuation)\n",
        "\n",
        "  # Menghapus tanda baca dari teks\n",
        "  text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
        "\n",
        "  return text_without_punctuation\n",
        "\n",
        "# Contoh teks dengan tanda baca\n",
        "teks_asli = \"Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\"\n",
        "\n",
        "# Menghapus tanda baca dari teks\n",
        "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
        "\n",
        "# Menampilkna hasil\n",
        "print(\"Teks asli:\", teks_asli)\n",
        "print(\"Teks setelah menghapus tanda baca:\", teks_tanpa_tanda_baca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UAWifZmtN-9",
        "outputId": "b7892fec-5ded-4be8-efb4-fd1fb333eb1d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\n",
            "Teks setelah menghapus tanda baca: Ini adalah contoh teks dengan tanda baca Contoh ini digunakan untuk demonstrasi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh lain dengan banyak tanda baca\n",
        "teks_asli = \"\"\"\n",
        "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
        "\"\"\"\n",
        "\n",
        "# Menghapus tanda dari teks\n",
        "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
        "\n",
        "# Menampilkna hasil\n",
        "print(\"Teks asli:\")\n",
        "print(teks_asli)\n",
        "print(\"\\nTeks setelah menghapus tanda baca:\")\n",
        "print(teks_tanpa_tanda_baca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBb6i_ruuLfs",
        "outputId": "3930c68c-eb21-4db1-e42e-b0d002721698"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli:\n",
            "\n",
            "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
            "\n",
            "\n",
            "Teks setelah menghapus tanda baca:\n",
            "\n",
            "Dalam dunia ini banyak hal terjadi dari yang kecil hingga yang besar kita bisa melihat keindahan tapi juga kekejaman Ada harapan namun juga keputusasaan Bagaimanapun hidup terus berjalan tak peduli apa pun yang terjadi\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghapus White Space"
      ],
      "metadata": {
        "id": "kPS6HxODwNYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menggunakan strip() untuk menghapus Whitespace di Awal dan Akhir"
      ],
      "metadata": {
        "id": "xorB2cmEwQB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks = \"  Ini adalah contoh teks dengan spasi di awal dan akhir.    \"\n",
        "teks_setelah_strip = teks.strip()\n",
        "print(teks_setelah_strip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUCvrbWGwM4n",
        "outputId": "42ef2ac3-fff2-411a-f8eb-aacd5038796a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ini adalah contoh teks dengan spasi di awal dan akhir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teks_dengan_whitespace = \"Ini adalah  contoh kalimat  dengan spasi  di dalamnya.\"\n",
        "teks_tanpa_whitespace = teks_dengan_whitespace.replace(\" \", \"\")\n",
        "print(teks_tanpa_whitespace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HaX1DikxQRj",
        "outputId": "b66b9a3f-7b2c-4f67-8346-48b9e23d9aed"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniadalahcontohkalimatdenganspasididalamnya.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal (Filtering)"
      ],
      "metadata": {
        "id": "agisrMu2xx5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords NLTK (Natural Language Toolkit)"
      ],
      "metadata": {
        "id": "dnawdh70x49k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')  # Untuk tokenisasi kata\n",
        "\n",
        "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
        "\n",
        "# Tokenisasi teks menjadi kata-kata\n",
        "tokens_kata = word_tokenize(teks)\n",
        "\n",
        "# Ambil daftar stopwords bahasa Indonesia dari NLTK\n",
        "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Filtering kata-kata dengan menghapus stopwords\n",
        "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
        "\n",
        "# Gabungkan kata-kata penting kembali menjadi teks\n",
        "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
        "\n",
        "print(\"Teks asli:\", teks)\n",
        "print(\"Teks setelah filtering stopwords NLTK:\", teks_tanpa_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVd_2u-B0Qaa",
        "outputId": "f6a42e9f-2031-4cdd-e02c-8c5906c1ebb1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
            "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  # Menga,mbil daftar stopword bahasa Indonesia dari NLTK\n",
        "  stop_words_indonesian = set(stopwords.words('indonesian'))\n",
        "\n",
        "  # Memisahkan teks menjadi kata-kata\n",
        "  words = text.split()\n",
        "\n",
        "  filtered_words = [word for word in words if word.lower() not in stop_words_indonesian]\n",
        "\n",
        "  # Menggabungkan kata-kata yang sudah difilter kembali menjadi kalimat\n",
        "  filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "  return filtered_text\n",
        "\n",
        "# Kalimat panjang sebelum penghapusan stopword\n",
        "kalimat_panjang = \"Pengembangan kecerdasan buatan (artificial intelligence atau AI) telah menjadi fokus utama dalam dunia teknologi. Teknologi AI memungkinkan komputer untuk belajar dari data, menemukan pola-pola kompleks, dan membuat keputusan seperti manusia. Penggunaan AI yang luas terlihat dalam berbagai bidang seperti pengolahan bahasa alami (NLP), pengenalan wajah, mobil otonom, analisis data, dan banyak lagi.\"\n",
        "\n",
        "\n",
        "# Memanggil fungsi untuk menghapus stopword pada kalimat panjang\n",
        "kalimat_panjang_filtered = remove_stopwords(kalimat_panjang)\n",
        "\n",
        "# Output teks setelah penghapusan stopword\n",
        "print(\"Kalimat panjang setelah penghapusan stopword:\", kalimat_panjang_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW1-gNkp0lHS",
        "outputId": "93894fda-18e5-46b4-8315-9c56e5efde72"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat panjang setelah penghapusan stopword: Pengembangan kecerdasan buatan (artificial intelligence AI) fokus utama dunia teknologi. Teknologi AI komputer belajar data, menemukan pola-pola kompleks, keputusan manusia. Penggunaan AI luas bidang pengolahan bahasa alami (NLP), pengenalan wajah, mobil otonom, analisis data, lagi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords Sastrawi"
      ],
      "metadata": {
        "id": "FzCmw40E2H8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Sastrawi library if not already installed\n",
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Phj5W62HSk",
        "outputId": "7f9b3946-4551-481f-afc8-1a2cbbb693ef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Inisialisasi objek StopWordRemover dari Sastrawi\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords_sastrawi = factory.get_stop_words()\n",
        "\n",
        "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
        "\n",
        "# Tokenisasi teks menjadi kata-kata\n",
        "tokens_kata = word_tokenize(teks)\n",
        "\n",
        "# Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
        "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
        "\n",
        "# Gabungkan kata-kata penting menjadi teks\n",
        "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
        "\n",
        "print(\"Teks asli:\", teks)\n",
        "print(\"Teks setelah filtering stopwords Sastrawi:\", teks_tanpa_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv_E7AIY2RHN",
        "outputId": "1792e377-1d54-4a19-8f84-68b64d22767a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
            "Teks setelah filtering stopwords Sastrawi: Perekonomian Indonesia sedang pertumbuhan membanggakan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "re4pahC2_HBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Kata (Word Tokenization)"
      ],
      "metadata": {
        "id": "68RBwKlu_I9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Ini adalah contoh kalimat untuk tokenisasi kata.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myxz3MHh_PCD",
        "outputId": "2ebf7307-f751-4f22-94d8-6894a2677128"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Kalimat (Sentence Tokenizzation)\n"
      ],
      "metadata": {
        "id": "Gfex7-2r_qdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XwhpagR_wwY",
        "outputId": "a7886433-02a9-4a56-e775-1858fc6ac307"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ini adalah contoh kalimat pertama.', 'Dan ini adalah contoh kalimat kedua.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Frasa (Phrase Tokenization)\n"
      ],
      "metadata": {
        "id": "AuCooVllANgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
        "text = \"Apel, jeruk, pisang, dan mangga.\"\n",
        "phrases = text.split(',')\n",
        "print(phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KAZeZUYAMRR",
        "outputId": "9a8a5e8e-4775-4c2f-9c34-fe202d3a9adf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apel', ' jeruk', ' pisang', ' dan mangga.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)"
      ],
      "metadata": {
        "id": "CMl7Z-SHAf76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh aturan tokenisasi khsus untuk tokenisasi kata dalam bahasa infonesia\n",
        "import re\n",
        "\n",
        "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
        "tokens = re.findall(r'\\w+|\\d+', text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff5pWlVCAk1m",
        "outputId": "16b50d2d-9d37-47b7-838e-8979c3265cea"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Berdasarkan Model (Model-based Tokenization)"
      ],
      "metadata": {
        "id": "nDpRymRiBYKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misalnya menggunakna spasi sebagai pemisah kata\n",
        "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XglMkTpwBdmj",
        "outputId": "f35b945e-b344-4318-856e-f4b48c98bf26"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "fSHNzHxoB81l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming menggunakan NLTK\n"
      ],
      "metadata": {
        "id": "8KFb_0ESB-aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xogBSzrCB7J",
        "outputId": "c8235d6e-e47a-498f-e45e-5b31a43edebd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'easili', 'bought', 'cri', 'leav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "54tHu1LSCqaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatisasi menggunakan NLTK"
      ],
      "metadata": {
        "id": "VZhU61orCtFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus  import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFzL_16YCw2i",
        "outputId": "737a4ac2-e60d-425c-f97a-dd791230f827"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'easily', 'buy', 'cry', 'leave']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kalimat contoh\n",
        "sentence = \"The cats are rinning and jumping over the fences.\"\n",
        "\n",
        "# Inisialisasi stemmer dan lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Lemmatisasi\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"Original tokens:\", tokens)\n",
        "print(\"Stemmed tokens:\", stemmed_words)\n",
        "print(\"Lemmatized tokens:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fubgxA5DXpn",
        "outputId": "fcdbd574-b507-4f91-bdd9-6b75e08ea567"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['The', 'cats', 'are', 'rinning', 'and', 'jumping', 'over', 'the', 'fences', '.']\n",
            "Stemmed tokens: ['the', 'cat', 'are', 'rin', 'and', 'jump', 'over', 'the', 'fenc', '.']\n",
            "Lemmatized tokens: ['The', 'cat', 'are', 'rinning', 'and', 'jumping', 'over', 'the', 'fence', '.']\n"
          ]
        }
      ]
    }
  ]
}